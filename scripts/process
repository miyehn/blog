#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import glob
import json
import os
import shutil
import time
import datetime

rawData = []
content_dir = os.environ['MRBLOG_CONTENT']
magicword = open(content_dir + '/magicword', 'r').read().strip()

#remove old files in posts/
paths = glob.glob(content_dir + '/blogposts/*')
for path in paths:
    os.remove(path)

paths = glob.glob(content_dir + '/index/*')
for path in paths:
    os.remove(path)

paths = glob.glob(content_dir + '/drafts/*.md')

# get timestamp (parsed from date)
def getTimeKey(obj):
    datestr = obj['date']
    timestamp = "unparsed"
    try:
        timestamp = time.mktime(datetime.datetime.strptime(datestr, "%Y-%m-%d %H:%M").timetuple())
    except:
        try:
            timestamp = time.mktime(datetime.datetime.strptime(datestr, "%Y-%m-%d %H:%M:%S").timetuple())
        except:
            timestamp = 99999999999999999
    return timestamp

# maintain a set of tags for later
listsPerTag = {}

for path in paths:
    file = open(path, 'r')
    text = file.read().strip()

    header = text[text.find('---\n')+4:]
    body = header[header.find('---\n')+4:]
    header = header[0:header.find('---\n')]

    # parse body to see if has partially hidden content
    ind = body.find('<!--hide-->')+11
    partial = False
    textOrig = ''
    textAlt = ''
    while ind >= 11:
        partial = True
        textOrig += body[0:ind-11]
        textAlt += body[0:ind-11]
        ind2 = body.find('<!--endhide-->')
        if ind2>=0:
            textAlt += body[ind:ind2]
            body = body[ind2 + 14:]
        else:
            textAlt += body[ind:]
            body = ''
        ind = body.find('<!--hide-->')+11
    
    textOrig += body
    textAlt += body

    # title
    ind = header.find('title:')+6
    if ind<6:
        title = ''
    else:
        title = header[ind:]
    title = title[0:title.find('\n')].strip()
    # tags
    ind = header.find('tags:')+5
    if ind<5:
        tagstr = ''
    else:
        tagstr = header[ind:]
    tagstr = tagstr[0:tagstr.find('\n')].strip()[1:-1]
    tagarr0 = tagstr.split(',')
    tagarr0 = filter(None, tagarr0)
    tagarr = []
    for tag in tagarr0:
        tagStripped = tag.strip()
        tagarr.append(tagStripped)

    # publicity
    ind = header.find('public:')+7
    if ind<5:
        publicity = 0
    elif header[ind:].strip()[0:4].lower()=='true':
        if partial:
            publicity = 1
        else:
            publicity = 2
    else:
        publicity = 0
    if publicity==0 or publicity==1:
        tagarr.append('hidden')

    # date
    ind = header.find('date:')+5
    if ind<5:
        datestr = ''
    else:
        datestr = header[ind:]
    datestr = datestr[0:datestr.find('\n')].strip()

    # jsonify
    data = {
        'title': title,
        'path': path[len(content_dir) + 8:-3],
        'tags': tagarr,
        'publicity': publicity,
        'date': datestr
    }

    # append to summary texts
    rawData.append(data)

    # append to tag lists
    for tag in tagarr:
        if not (tag in listsPerTag): listsPerTag[tag] = []
        listsPerTag[tag].append(data)

    # rewrite file content + magic
    path = content_dir + '/blogposts' + path[len(content_dir) + 7:]
    if publicity==2: # public
        with open(path[:-3], 'w+') as outfile:
            outfile.write(textOrig.strip())
    if publicity==1: # partially public
        with open(path[:-3]+'-'+magicword, 'w+') as outfile:
            outfile.write(textAlt.strip())
    else: # hidden
        with open(path[:-3], 'w+') as outfile:
            outfile.write(textAlt.strip())

    file.close()

# @data: list to output into chunks
# @chunkSize: number of posts to output per chunk
def writeLists(data, outPrefix, chunkSize):
    data.sort(reverse=True, key=getTimeKey)
    numChunks = int((len(data) + chunkSize - 1) / chunkSize)
    for chunkIndex in range(0, numChunks):
        # get sublist
        lo = chunkIndex * chunkSize
        hi = lo + chunkSize
        if hi > len(data): hi = len(data)
        sublist = data[lo:hi]
        # process sublist
        with open (outPrefix + str(chunkIndex), 'w+') as outfile:
            json.dump(sublist, outfile, separators=(',',':'), ensure_ascii=False)
            #json.dump(rawData, outfile, indent=2, ensure_ascii=False)


# TODO: public linked list: no publicity info; other linked list: with publicity info
publicData = list(filter(lambda obj: obj['publicity'] == 2, rawData))

# all (as seen from blog home page)
writeLists(rawData, content_dir + '/index/' + magicword + '_all_', 50)
writeLists(publicData, content_dir + '/index/all_', 50)

# tags.. (filter by each, output two respectively)
for tag in listsPerTag:
    publicTagData = list(filter(lambda obj: obj['publicity'] == 2, listsPerTag[tag]))
    writeLists(listsPerTag[tag], content_dir + '/index/' + magicword + '_tag_' + tag + '_', 50)
    writeLists(publicTagData, content_dir + '/index/tag_' + tag + '_', 50)

print('summary generated')

